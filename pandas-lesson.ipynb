{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**What is Data Science and what is pandas?**\nData Science or data analytics is a process of analyzing large set of data points to get answers on quaestions related to that data set.\nPandas is a python module that makes data science easy and effective!\n","metadata":{"id":"jg1D9x-AYxf7"}},{"cell_type":"code","source":"import pandas as pd","metadata":{"id":"5XhCVMynZ3ci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Process of cleaning messy data is called data munging or data wrangling","metadata":{"id":"bOr-_iGbbH-M"}},{"cell_type":"markdown","source":"**Dataframe** is main object in Pandas.It is used to represent data with rows and collumns (tabular or excel spreadsheet like data)","metadata":{"id":"mbMtMDxQcDCu"}},{"cell_type":"markdown","source":"How to get data from googledrive with googlecollab","metadata":{"id":"zfLyqveJigYg"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')","metadata":{"id":"_4y0LMZ7cqi2","executionInfo":{"status":"ok","timestamp":1698240720504,"user_tz":-180,"elapsed":25085,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"1455a844-d3a4-4a43-9e9f-c755e9f2fdb3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_excel('/content/gdrive/My Drive/Data Science/Practice/weather_data.xlsx')","metadata":{"id":"8z8y_GP3epTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"OT0WNRf0jbqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print shape, rows and collumns\nrows, columns = df.shape\nprint(df.shape)\nprint(rows)\nprint(columns)","metadata":{"id":"3naBeJr_h-Sf","executionInfo":{"status":"ok","timestamp":1697967417233,"user_tz":-180,"elapsed":262,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"479baaf1-65f3-4b65-f54c-64d4c96a605e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print specific number of rows from start\nprint(df.head(2))\n\n# Print specific number of rows from last\nprint(df.tail(2))\n\n# Print specific number of rows\nprint(df[2:5])","metadata":{"id":"5lME8or3jrmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print specific collumn by name\n\nprint(df.day)\nprint(\"\")\n# or\nprint(df[\"day\"])\nprint(\"\")\n\n# Print specific collumns by name\nprint(df[[\"day\", \"temperature\"]])","metadata":{"id":"2Gt70EvemGEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print types\n\nprint(type(df[\"day\"]))","metadata":{"id":"MH47nUeOmlx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Operations with dataframes**","metadata":{"id":"u80BQEKMnNvY"}},{"cell_type":"code","source":"# Print maximum, minimum, average, standard deviation of temperature\n\nprint(df[\"temperature\"].max())\nprint(\"\")\nprint(df[\"temperature\"].min())\nprint(\"\")\nprint(df[\"temperature\"].mean())\nprint(\"\")\nprint(df[\"temperature\"].std())\nprint(\"\")","metadata":{"id":"A3Uq9ypunVfF","executionInfo":{"status":"ok","timestamp":1698151406014,"user_tz":-180,"elapsed":243,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"c91db463-893a-44a9-e5c5-6d3808d404b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print statistics in collumns with \"numbers\"\n\nprint(df.describe())","metadata":{"id":"WMRWcz9AoCgA","executionInfo":{"status":"ok","timestamp":1698151411060,"user_tz":-180,"elapsed":258,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"03e0b6f7-0447-463f-c3b0-28c305b4f1c2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conditionally select rows where temp >= 32\n\nprint(df[df.temperature>=32])\nprint(\"\")\n\n# Give rows where temperature is maximum\n# Give from these rows only day and temperature collumn where temperature is maximum\n\nprint(df[df.temperature ==  df[\"temperature\"].max()])\nprint(\"\")\nprint(df[[\"day\", \"temperature\"]][df.temperature ==  df[\"temperature\"].max()])\nprint(\"\")\n","metadata":{"id":"j9bbhqssoR8i","executionInfo":{"status":"ok","timestamp":1698151417828,"user_tz":-180,"elapsed":230,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"18b81ba1-cc34-4f0e-9a3b-2e3080706223"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Indexing of dataframe**","metadata":{"id":"PHUL-wSYvN7x"}},{"cell_type":"code","source":"print(df)\nprint(\"\")\nprint(df.index)\nprint(\"\")\n\n# Change index and save it to the original\ndf.set_index(\"day\", inplace=True)\nprint(df)\n","metadata":{"id":"Fvdw3ZA0vP4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset index to default\ndf.reset_index(inplace=True)\nprint(df)","metadata":{"id":"5VdCVVzqxgFX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Different Ways Of Creating Dataframe**\n\n\n1.   Using CSV\n2.   Using Excel\n3.   From Python Dictionary\n4.   From List Of Tuples\n5.   From List of Dictionaries\n\n","metadata":{"id":"8tkgvClPyU5m"}},{"cell_type":"code","source":"import pandas as pd\n\n#1\ndf1 = pd.read_csv('/content/gdrive/My Drive/Data Science/Practice/weather_data2.csv')\nprint(df1)\nprint(\"\")\n#2\ndf2 = pd.read_excel('/content/gdrive/My Drive/Data Science/Practice/weather_data.xlsx')\nprint(df1)\nprint(\"\")\n\n#3\nweather_data = {\n'day': ['1/1/2017', '1/2/2017', '1/3/2017'],\n'temperature': [32, 35, 28],\n'windspeed': [6,7,2],\n'event': ['Rain', 'Sunny', 'Snow']\n}\n\ndf3 = pd.DataFrame(weather_data)\nprint(df3)\nprint(\"\")\n\n#4\nweather_data = [\n('1/1/2017', 32, 6, 'Rain'),\n('1/2/2017', 35, 7, 'Sunny'),\n('1/3/2017', 28, 2, 'Snow')\n]\n\ndf4 = pd.DataFrame(weather_data, columns=[\"day\", \"temperature\", \"windspeed\", \"event\"])\nprint(df4)\nprint(\"\")\n\n#5\nweather_data = [\n{'day': '1/1/2017', 'temperature': 32, 'windspeed': 6, 'event': 'Rain'},\n{'day': '1/2/2017', 'temperature': 35, 'windspeed': 7, 'event': 'Sunny'},\n{'day': '1/3/2017', 'temperature': 28, 'windspeed': 2, 'event': 'Snow'}\n]\ndf5 = pd.DataFrame(weather_data)\nprint(df5)\nprint(\"\")\n\n","metadata":{"id":"yynpacGhyuf3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"GC4Z_Owa3-wk"}},{"cell_type":"markdown","source":"**Reading writing csv, excel files**\n\n1.   Read csv\n2.   Write csv\n3.   Read excel\n4.   Write excel\n\n","metadata":{"id":"IXlJQ3UZ4ULp"}},{"cell_type":"markdown","source":"**CSV**","metadata":{"id":"a7Om5X6jdjWd"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')\n\nimport pandas as pd\ndf = pd.read_csv('/content/gdrive/My Drive/Data Science/Practice/stock_data.csv')\nprint(df)\n\n","metadata":{"id":"2GW8xh-EZddq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skip rows in dataframe using skiprows\ndf = pd.read_csv('/content/gdrive/My Drive/Data Science/Practice/stock_data.csv', skiprows=1)\nprint(df)\n\n","metadata":{"id":"Zr_fxhOX9DUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read limited data from dataframe with nrows    ex. only 3 first rows excluding header\ndf = pd.read_csv('/content/gdrive/My Drive/Data Science/Practice/stock_data.csv', nrows=3)\nprint(df)\n\n","metadata":{"id":"9LO1gXOY9DLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleanup messy data (n.a. values in all collumns and -1 in revenue collumn)\ndf = pd.read_csv('/content/gdrive/My Drive/Data Science/Practice/stock_data.csv', na_values={\n    \"eps\" : [\"not available\", \"n.a.\"],\n    \"revenue\" : [\"not available\", \"n.a.\", -1],\n    \"price\" : [\"not available\", \"n.a.\"],\n    \"people\" : [\"not available\", \"n.a.\"]\n})\nprint(df)\nprint(\"\")","metadata":{"id":"17h9KpSW9DES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write back to csv\n\ndf.to_csv('/content/gdrive/My Drive/Data Science/Practice/new_stock_data.csv')\n","metadata":{"id":"Mhby-7Es9C7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EXCEL**","metadata":{"id":"Ecpn2Wnmgzen"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')","metadata":{"id":"w97BI0r9hVKg","executionInfo":{"status":"ok","timestamp":1698240748704,"user_tz":-180,"elapsed":2867,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"339d067c-11e6-4ee3-cac2-c7023abd1e44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# convert arguments (in this example in collumn people, n.a. with sam walton)\n\ndef convert_people_cell(cell):\n  if cell == \"n.a.\":\n    return \"sam walton\"\n  else:\n    return cell\n\ndf = pd.read_excel('/content/gdrive/My Drive/Data Science/Practice/stock_data.xlsx', converters = {\n    \"people\": convert_people_cell\n})\nprint(df)\nprint(\"\")\n\n# write to excel and put sheet name\n\ndf.to_excel('/content/gdrive/My Drive/Data Science/Practice/new_stock_data.xlsx', sheet_name=\"stocks\")\n\n\n","metadata":{"id":"WOglf7Ghg1F2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write 2 dataframes in 1 excel file  (in different sheets)\n\ndf_stocks = pd.DataFrame({\n    'tickers': ['GOOGL', 'WMT', 'MSFT'],\n    'price': [845, 65, 64 ],\n    'pe': [30.37, 14.26, 30.97],\n    'eps': [27.82, 4.61, 2.12]\n})\n\ndf_weather =  pd.DataFrame({\n    'day': ['1/1/2017','1/2/2017','1/3/2017'],\n    'temperature': [32,35,28],\n    'event': ['Rain', 'Sunny', 'Snow']\n})\n\nwith pd.ExcelWriter('/content/gdrive/My Drive/Data Science/Practice/stocks_weather.xlsx') as writer:\n    df_stocks.to_excel(writer, sheet_name=\"stocks\")\n    df_weather.to_excel(writer, sheet_name=\"weather\")","metadata":{"id":"Qbf1LGGOkpK6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Handle Missing Data: fillna, dropna, interpolate**\n1. fillna to fill missing values using differente ways\n2. interpolate to make a guess on missing values using interpolation\n3. dropna to drop rows with missing values","metadata":{"id":"15zu5ht7lar0"}},{"cell_type":"code","source":"import pandas as pd\n\n# import and convert dates\ndf = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Practice/weather_data_f.csv\", parse_dates=[\"day\"])\n# set index on collumn day\ndf.set_index(\"day\",inplace=True)\nprint(df)\n\n\n\n","metadata":{"id":"LrBoh8ERmUXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace all NaN values\n\nnew_df = df.fillna(0)\nprint(new_df)\nprint(\"\")","metadata":{"id":"iXKjsGdS9YvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaN values but specifically in collumns\n\nnew_df2 = df.fillna({\n    \"temperature\": 0,\n    \"windspeed\": 0,\n    \"event\": \"no event\"\n})\nprint(new_df2)","metadata":{"id":"LcVDcDQy9ZXh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaN values with forward fill method (check previous value to fill the NaN)\n\nnew_df3 = df.fillna(method=\"ffill\")\nprint(new_df3)","metadata":{"id":"zbGkcD9d9ZQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaN values with backward fill method (check next value to fill the NaN)\n\nnew_df4 = df.fillna(method=\"bfill\")\nprint(new_df4)","metadata":{"id":"vU5Q7Phi9ZM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaN values with backward fill method (check next value to fill the NaN)\n# and now horizontally\n\nnew_df5 = df.fillna(method=\"bfill\", axis=\"columns\")\nprint(new_df5)","metadata":{"id":"U0qGcz11ArGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace NaN values with forward fill method (check previous value to fill the NaN)\n# and also put limit for only 1 of next value\n\nnew_df6 = df.fillna(method=\"bfill\", limit = 1)\nprint(new_df6)","metadata":{"id":"XZARFfppBTBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# guess values with interpolate()       {default method = Linear interpolation}\n\nnew_df7 = df.interpolate()\nprint(new_df7)","metadata":{"id":"R3ZVwXZtBpun"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use interpolate with interpolate() change method to time\n\nnew_df8 = df.interpolate(method=\"time\")\nprint(new_df8)","metadata":{"id":"N3-sGTAfCSs8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use dropna() to drop every row with at least one NaN\n\nnew_df9 = df.dropna()\nprint(new_df9)","metadata":{"id":"NObsielECyN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use dropna() to drop every row if all values are NaN\n\nnew_df10 = df.dropna(how=\"all\")\nprint(new_df10)","metadata":{"id":"z6mk6Gl6DlP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use dropna() to key every row if there is at least one valid value\n\nnew_df11 = df.dropna(thresh=1)\nprint(new_df11)","metadata":{"id":"qsS1EhZqEAR-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inserting missing dates\n\ndt = pd.date_range(\"01-01-2017\",\"01-11-2017\")\nidx = pd.DatetimeIndex(dt)\ndf = df.reindex(idx)\n\nprint(df)","metadata":{"id":"rEToATWlEnAZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Handle Missing Data with replace function**","metadata":{"id":"JJ2hnAv-FuvR"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Practice/weather_data_b.csv\")\nprint(df)\n\n# replace function all values in every collumn equal to -99999\nnew_df1 = df.replace(-99999,np.NaN)\nprint(new_df1)","metadata":{"id":"GNuYuv4_GELJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace function to specific values in each collum\nnew_df2 = df.replace({\n    \"temperature\": -99999,\n    \"windspeed\": -99999,\n    \"event\": \"0\"\n},np.NaN)\nprint(new_df2)","metadata":{"id":"wdUJYUM7HjUt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace function with mapping\nnew_df3 = df.replace({\n    -99999: np.NaN,\n    \"0\": \"Sunny\"\n})\nprint(new_df3)","metadata":{"id":"NR-2M6LqIqPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# regex           {Regular Expression}\n# replaces if it finds space between characters & characters\n# but we dont want to touch event collumn\n\nnew_df4 = df.replace({\n    \"temperature\": \"[A-Za-z]\",\n    \"windspeed\" : \"[A-Za-z]\"\n},\"\",regex=True)\nprint(new_df4)","metadata":{"id":"Deydlo7-JZVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace strings in collumn with numbers\n# for example in score collumn\nnew_df5 = pd.DataFrame({\n    'score': ['exceptional','average', 'good', 'poor', 'average', 'exceptional'],\n    'student': ['rob', 'maya', 'parthiv', 'tom', 'julian', 'erica']\n})\nprint(new_df5)\nprint(\"\")\nnew_df6 = new_df5.replace([\"poor\", \"average\", \"good\", \"exceptional\"], [1,2,3,4])\nprint(new_df6)","metadata":{"id":"fFtg3xr_L2W6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Group By (Split Apply Combine)**","metadata":{"id":"Zp2ZM4xuJK7u"}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Practice/weather_by_cities.csv\")\nprint(df)\n","metadata":{"id":"ELhlEfaTJPXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by all cities with dataframes     city, city_df\n\ng = df.groupby(\"city\")\nfor city, city_df in g:\n  print(city)\n  print(city_df)\n","metadata":{"id":"U9K-xHxTJcAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get specific city from group\n\nprint(g.get_group(\"mumbai\"))","metadata":{"id":"Wp6g00MLK_EK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find max - min  in every collumns per city\n\nprint(g.max())\nprint(\"\")\nprint(g.min())","metadata":{"id":"L7eaQzreJfW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find average in every collumns per city\n\nprint(g.mean())","metadata":{"id":"3f_KpeaYLw_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all statistics in every collumns per city\n\nprint(g.describe())","metadata":{"id":"9q--WlLSM7pQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get charts\n\n%matplotlib inline\ng.plot()","metadata":{"id":"H4D3ShesNLTN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Concat Dataframes**","metadata":{"id":"j15y3z2UOFfX"}},{"cell_type":"code","source":"import pandas as pd\n\nindia_weather = pd.DataFrame({\n    \"city\": [\"mumbai\",\"delhi\",\"banglore\"],\n    \"temperature\": [32,45,30],\n    \"humidity\": [80, 60, 78]\n})\n\nprint(india_weather)\nprint(\"\")\n\nusa_weather = pd.DataFrame({\n    \"city\": [\"new tork\",\"chicago\",\"orlando\"],\n    \"temperature\": [21,14,35],\n    \"humidity\": [68, 65, 75]\n})\n\nprint(usa_weather)\nprint(\"\")","metadata":{"id":"XHoXYByfOMzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concat two dataframes with concat()\n\ndf = pd.concat([india_weather, usa_weather])\nprint(df)","metadata":{"id":"1TWfBd6UAAQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concat two dataframes with concat() and fix index\n\ndf = pd.concat([india_weather, usa_weather], ignore_index=True)\nprint(df)","metadata":{"id":"LsACCacuAds4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concat two dataframes with concat() and give keys of country\n\ndf = pd.concat([india_weather, usa_weather], keys=[\"india\", \"usa\"])\nprint(df)","metadata":{"id":"2N3UpgT9BAGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# retrieve subset of dataframe  ex. india_weather\n\nprint(df.loc[\"usa\"])","metadata":{"id":"bWTCveGdBNAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use case of 2 dataframes with different collumns\n\ntemperature_df = pd.DataFrame({\n    \"city\": [\"mumbai\",\"delhi\",\"banglore\"],\n    \"temperature\": [32,45,30]\n})\nwindspeed_df = pd.DataFrame({\n    \"city\": [\"mumbai\",\"delhi\",\"banglore\"],\n    \"windspeed\": [7,12,9]\n})\nprint(temperature_df)\nprint(\"\")\nprint(windspeed_df)\nprint(\"\")\n\n# concat these 2 dataframes with concat() with axis\n\ndf = pd.concat([temperature_df, windspeed_df], axis=1)\nprint(df)","metadata":{"id":"gp4_TWskBYiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat these 2 dataframes (different order in names, and less cities) with index\n\ntemperature_df = pd.DataFrame({\n    \"city\": [\"mumbai\",\"delhi\",\"banglore\"],\n    \"temperature\": [32,45,30]\n}, index=[0,1,2]) # we give index values\nwindspeed_df = pd.DataFrame({\n    \"city\": [\"delhi\", \"mumbai\" ],\n    \"windspeed\": [7,12]\n}, index=[1,0]) # we give same index values with temperature cities index numbers\nprint(temperature_df)\nprint(\"\")\nprint(windspeed_df)\nprint(\"\")\n\ndf = pd.concat([temperature_df, windspeed_df], axis=1)\nprint(df)","metadata":{"id":"TwnAB1oZCvzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat these 2 dataframes\n\ntemperature_df = pd.DataFrame({\n    \"city\": [\"mumbai\",\"delhi\",\"banglore\"],\n    \"temperature\": [32,45,30]\n})\n\ns = pd.Series([\"Rain\", \"Dry\", \"Rain\"], name=\"event\")\n\nprint(temperature_df)\nprint(\"\")\nprint(s)\nprint(\"\")\n\ndf = pd.concat([temperature_df, s], axis =1)\nprint(df)","metadata":{"id":"hqKNLZOfDjQ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merge Dataframes**","metadata":{"id":"Wwx-oUZTEQUJ"}},{"cell_type":"code","source":"import pandas as pd\n\ndf1 = pd.DataFrame({\n    \"city\": [\"new york\",\"chicago\",\"orlando\"],\n    \"temperature\": [21,14,35],\n})\ndf2 = pd.DataFrame({\n    \"city\": [\"chicago\",\"new york\",\"orlando\"],\n    \"humidity\": [65,68,75],\n})\nprint(df1)\nprint(\"\")\nprint(df2)\nprint(\"\")\n\n# merge these two dataframes based on cities\n\ndf = pd.merge(df1, df2, on=\"city\")\nprint(df)","metadata":{"id":"5BFBKpM8ESeB","executionInfo":{"status":"ok","timestamp":1698243915521,"user_tz":-180,"elapsed":4,"user":{"displayName":"Panagiotis Prassas","userId":"03773480548534920127"}},"outputId":"b9f725ef-43eb-44bd-bb86-ddfcdfd77745"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use case of two dataframes with different number of cities and different names of cities\ndf1 = pd.DataFrame({\n    \"city\": [\"new york\",\"chicago\",\"orlando\", \"baltimore\"],\n    \"temperature\": [21,14,35,32],\n})\ndf2 = pd.DataFrame({\n    \"city\": [\"chicago\",\"new york\",\"san fransisco\"],\n    \"humidity\": [65,68,71],\n})\nprint(df1)\nprint(\"\")\nprint(df2)\nprint(\"\")\n\n# merge these two dataframes based on cities with parameter how (inner, outer, left or right)\n# and parameter indicator to check from where data came\n\ndf3 = pd.merge(df1, df2, on=\"city\", how = \"outer\", indicator=True)\nprint(df3)","metadata":{"id":"TTbXcrlPFC2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf1 = pd.DataFrame({\n    \"city\": [\"new york\",\"chicago\",\"orlando\", \"baltimore\"],\n    \"temperature\": [21,14,35,38],\n    \"humidity\": [65,68,71, 75]\n})\ndf2 = pd.DataFrame({\n    \"city\": [\"chicago\",\"new york\",\"san diego\"],\n    \"temperature\": [21,14,35],\n    \"humidity\": [65,68,71]\n})\nprint(df1)\nprint(\"\")\nprint(df2)\nprint(\"\")\n\n# merge these two dataframes with suffixes parameter to see where the\n# data comes from\n\ndf3 = pd.merge(df1, df2, on = \"city\", suffixes = (\"left\", \"right\"))\nprint(df3)","metadata":{"id":"UtfyGy48FKwf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pivot and pivot_table**\n\nPivot allows you to transform or reshape data","metadata":{"id":"Vzf1JyfDJZ-E"}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Practice/weather.csv\")\nprint(df)\nprint(\"\")\n\n# use pivot() with index of row -> dates and columns -> city\n# and provide only humidity\n\ndf1 = df.pivot(index=\"date\", columns=\"city\", values=\"humidity\")\nprint(df1)","metadata":{"id":"3gDF2t3TJZRA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pivot table is used to summarize and aggregate data inside dataframe","metadata":{"id":"Yy9QpoPFO025"}},{"cell_type":"code","source":"# pivot table\n\ndf = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Practice/weather2.csv\")\nprint(df)\nprint(\"\")\n\n# use pivot table to take average temperature in same date\n# where rows are cities and collumn are dates\n# take average of same days values\n\ndf2 = df.pivot_table(index=\"city\", columns=\"date\")\nprint(df2)\n\n# use pivot table to take average temperature in same date\n# where rows are cities and collumn are dates\n# and use aggregate for sum ( by default is mean() )\n\ndf2_new = df.pivot_table(index=\"city\", columns=\"date\", aggfunc=\"sum\")\nprint(df2_new)\n","metadata":{"id":"Qpo_8VrhO4l9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouper\n\ndf = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Practice/weather3.csv\")\nprint(df)\nprint(\"\")\n\n# pivot to see month average temperatures with grouper()\n# first we need to convert string dates to number dates\n# freq \"W\" for weekly, \"M\" for monthly and \"Y\" for yearly average values\n\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\ndf3 = df.pivot_table(index=pd.Grouper(freq=\"M\", key=\"date\"), columns=\"city\")\nprint(df3)\n","metadata":{"id":"W7cSCy2IQynf"},"execution_count":null,"outputs":[]}]}